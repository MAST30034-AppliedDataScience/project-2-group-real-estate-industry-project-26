{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25493ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from time import sleep\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73d0e32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_property_info(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    property_info = {}\n",
    "\n",
    "    # 提取 JSON-LD 数据\n",
    "    json_ld = soup.find('script', type='application/ld+json')\n",
    "    if json_ld:\n",
    "        try:\n",
    "            json_data = json.loads(json_ld.string)\n",
    "            if not isinstance(json_data, list):\n",
    "                json_data = [json_data]\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON-LD data from {url}: {e}\")\n",
    "            print(\"JSON-LD content:\", json_ld.string[:500])\n",
    "            json_data = []\n",
    "    else:\n",
    "        print(f\"No JSON-LD data found in {url}\")\n",
    "        json_data = []\n",
    "\n",
    "    # 提取 digitalData\n",
    "    digital_data_script = soup.find('script', string=re.compile('var digitalData'))\n",
    "    if digital_data_script:\n",
    "        digital_data_match = re.search(r'var digitalData = (.+?);', digital_data_script.string, re.DOTALL)\n",
    "        if digital_data_match:\n",
    "            try:\n",
    "                digital_data = json.loads(digital_data_match.group(1))\n",
    "                #print(f\"Digital data structure: {json.dumps(digital_data, indent=2)[:500]}...\")  # 打印数据结构\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding digitalData from {url}: {e}\")\n",
    "                print(\"digitalData content:\", digital_data_match.group(1)[:500])\n",
    "                digital_data = {}\n",
    "        else:\n",
    "            print(f\"digitalData pattern not found in {url}\")\n",
    "            digital_data = {}\n",
    "    else:\n",
    "        print(f\"digitalData script not found in {url}\")\n",
    "        digital_data = {}\n",
    "\n",
    "    # 提取属性数据，添加更多的错误检查\n",
    "    page_info = digital_data.get('page', {})\n",
    "    if not isinstance(page_info, dict):\n",
    "        print(f\"Unexpected 'page' structure in digital_data: {type(page_info)}\")\n",
    "        page_info = {}\n",
    "\n",
    "    property_data = page_info.get('pageInfo', {}).get('property', {})\n",
    "    if not isinstance(property_data, dict):\n",
    "        print(f\"Unexpected 'property' structure in digital_data: {type(property_data)}\")\n",
    "        property_data = {}\n",
    "\n",
    "    # 安全地获取属性\n",
    "    property_info['price'] = property_data.get('price')\n",
    "    property_info['area'] = 'Not available'\n",
    "    property_info['bedrooms'] = property_data.get('bedrooms')\n",
    "    property_info['bathrooms'] = property_data.get('bathrooms')\n",
    "    property_info['parking'] = property_data.get('parking')\n",
    "    property_info['agency'] = {\n",
    "        'id': property_data.get('agencyId'),\n",
    "        'name': property_data.get('agency')\n",
    "    }\n",
    "    property_info['nbn_type'] = page_info.get('pageInfo', {}).get('nbnDetails')\n",
    "    property_info['property_type'] = property_data.get('primaryPropertyType')\n",
    "\n",
    "    # 地理坐标\n",
    "    for item in json_data:\n",
    "        if isinstance(item, dict) and item.get('@type') == 'Event' and 'location' in item:\n",
    "            geo = item['location'].get('geo', {})\n",
    "            property_info['geo'] = {\n",
    "                'latitude': geo.get('latitude'),\n",
    "                'longitude': geo.get('longitude')\n",
    "            }\n",
    "            break\n",
    "    \n",
    "    # 提取地址信息\n",
    "    for item in json_data:\n",
    "        if item.get('@type') == 'Residence':\n",
    "            address = item.get('address', {})\n",
    "            property_info['location'] = {\n",
    "                'streetAddress': address.get('streetAddress'),\n",
    "                'addressLocality': address.get('addressLocality'),\n",
    "                'addressRegion': address.get('addressRegion'),\n",
    "                'postalCode': address.get('postalCode')\n",
    "            }\n",
    "            break        \n",
    "    \n",
    "    # 提取学校信息\n",
    "    schools = []\n",
    "    school_elements = soup.find_all(['div', 'label'], class_=['css-1eyghyo', 'domain-checkbox'])\n",
    "    for school in school_elements:\n",
    "        school_name = school.find(['h4', 'div'], class_=['css-5w5cop', 'domain-checkbox__label'])\n",
    "        if school_name:\n",
    "            schools.append(school_name.text.strip())\n",
    "    property_info['nearby_schools'] = list(set(schools))\n",
    "\n",
    "    # 提取邻里年龄分布信息\n",
    "    age_distribution = {}\n",
    "    age_rows = soup.find_all('tr', class_='css-1a43shy')\n",
    "    for row in age_rows:\n",
    "        age_range = row.find('td', class_='css-1srjr3j')\n",
    "        percentage = row.find('div', class_='css-199ul8s')\n",
    "        if age_range and percentage:\n",
    "            age_distribution[age_range.text.strip()] = percentage.text.strip()\n",
    "    property_info['age_distribution'] = age_distribution\n",
    "\n",
    "    return property_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0af6d7",
   "metadata": {},
   "source": [
    "url = 'https://www.domain.com.au/9b-131-lonsdale-street-melbourne-vic-3000-17186660'\n",
    "property_info = extract_property_info(url)\n",
    "print(property_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3df4a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_property_urls(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    property_urls = []\n",
    "\n",
    "    # 尝试从JSON-LD数据中提取URL\n",
    "    json_ld_scripts = soup.find_all('script', type='application/ld+json')\n",
    "    for script in json_ld_scripts:\n",
    "        try:\n",
    "            data = json.loads(script.string)\n",
    "            if isinstance(data, list):\n",
    "                for item in data:\n",
    "                    if item.get('@type') == 'Event' and 'url' in item:\n",
    "                        property_urls.append(item['url'])\n",
    "            elif isinstance(data, dict) and data.get('@type') == 'Event' and 'url' in data:\n",
    "                property_urls.append(data['url'])\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "    all_links = soup.find_all('a', href=True)\n",
    "    domain_pattern = re.compile(r'https?://www\\.domain\\.com\\.au/.*-\\d+$')\n",
    "\n",
    "    for link in all_links:\n",
    "        href = link['href']\n",
    "        # 检查链接是否匹配domain.com.au的房产URL模式\n",
    "        if domain_pattern.match(href):\n",
    "            property_urls.append(href)\n",
    "        # 处理相对URL\n",
    "        elif href.startswith('/') and '-' in href and href.split('-')[-1].isdigit():\n",
    "            full_url = f\"https://www.domain.com.au{href}\"\n",
    "            if full_url.endswith('/'):\n",
    "                property_urls.append(full_url)\n",
    "            else:\n",
    "                property_urls.append(full_url + '/')\n",
    "\n",
    "    \n",
    "\n",
    "    # 去除重复的URL\n",
    "    property_urls = list(set(property_urls))\n",
    "    \n",
    "    filtered_urls = [\n",
    "        url for url in property_urls \n",
    "        if re.search(r'/[\\w-]+-\\d+$', url) and 'suburb-profile' not in url\n",
    "    ]\n",
    "    \n",
    "    return filtered_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07e823c",
   "metadata": {},
   "source": [
    "base_url = \"https://www.domain.com.au/rent/vic/\"\n",
    "urls = extract_property_urlss(base_url)\n",
    "#print(json.dumps(urls, indent=2))\n",
    "print(len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58b5e67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_csv(property_l, output_path):\n",
    "    # 确保property_list是一个列表\n",
    "    if isinstance(property_l, str):\n",
    "        property_l = ast.literal_eval(property_l)\n",
    "        \n",
    "    property_l = [p for p in property_l if p is not None]\n",
    "\n",
    "    if not property_l:\n",
    "        print(\"No valid properties to write to CSV.\")\n",
    "        return\n",
    "    \n",
    "    fieldnames = set()\n",
    "    for property_info in property_l:\n",
    "        if isinstance(property_info, dict):\n",
    "            fieldnames.update(property_info.keys())\n",
    "        else:\n",
    "            print(f\"Skipping non-dict item: {property_info}\")\n",
    "\n",
    "    # 确保输出目录存在\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    # 写入CSV文件\n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        # 写入表头\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # 写入每个属性的信息\n",
    "        for property_info in property_l:\n",
    "            writer.writerow(property_info)\n",
    "\n",
    "    print(f\"CSV file has been created at: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0cc039",
   "metadata": {},
   "source": [
    "\n",
    "output_path = \"../data/landing/properties.csv\"\n",
    "\n",
    "#### 调用函数\n",
    "convert_property_list_to_csv(a, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a70f20de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_properties(base_url, page_l, output_path):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "    url_l = []\n",
    "    property_l = []\n",
    "    \n",
    "    for base_url in base_url_l:\n",
    "        response = requests.get(base_url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            for page in page_l:\n",
    "                url = base_url + str(page)\n",
    "                try:\n",
    "                    response = requests.get(url, headers=headers, timeout=10)\n",
    "                    response.raise_for_status()\n",
    "                    new_urls = extract_property_urls(url)\n",
    "                    if not new_urls:\n",
    "                        break\n",
    "                    url_l.extend(new_urls)\n",
    "                except RequestException as e:\n",
    "                    break\n",
    "           # time.sleep(1)\n",
    "            \n",
    "            \n",
    "       #     for page in page_l:\n",
    "        #        url = base_url + str(page)\n",
    "      #          response = requests.get(base_url, headers=headers)\n",
    "       #         if response.status_code == 200:\n",
    "        #            url_l.extend(extract_property_urls(url))\n",
    "      #          else:\n",
    "       #             break\n",
    "           \n",
    "        print(f\"number of url = {len(url_l)}\")\n",
    "    url_l = list(set(url_l))\n",
    "    \n",
    "    cont = 0\n",
    "    for a_url in url_l:\n",
    "        cont += 1\n",
    "        property_l.append(extract_property_info(a_url))\n",
    "        if cont % 200 == 0:\n",
    "            print(f\"finish {cont}\")\n",
    "    \n",
    "    return property_l    \n",
    "    #convert_to_csv(property_l, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6726ecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url_l = []\n",
    "with open('../data/landing/victoria_suburbs_postcodes.csv', 'r', newline='', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # 跳过标题行\n",
    "    for row in reader:\n",
    "        if len(row) >= 2:\n",
    "            suburb, postcode = row[0], row[1]\n",
    "            suburb_url = suburb.lower().replace(' ', '-')\n",
    "            url = f\"https://www.domain.com.au/rent/{suburb_url}-vic-{postcode}/?ssubs=0&page=\"\n",
    "            base_url_l.append(url)\n",
    "#print(base_url_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb71af8c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of url = 600\n",
      "number of url = 600\n",
      "number of url = 635\n",
      "number of url = 727\n",
      "number of url = 834\n",
      "number of url = 1236\n",
      "number of url = 1436\n",
      "number of url = 1567\n",
      "number of url = 1583\n",
      "number of url = 1640\n",
      "number of url = 1665\n",
      "number of url = 1677\n",
      "number of url = 1719\n",
      "number of url = 1747\n",
      "number of url = 1765\n",
      "number of url = 1805\n",
      "number of url = 1818\n",
      "number of url = 1941\n",
      "number of url = 1975\n",
      "number of url = 2024\n",
      "number of url = 2059\n",
      "number of url = 2083\n",
      "number of url = 2112\n",
      "number of url = 2126\n",
      "number of url = 2188\n",
      "number of url = 2216\n",
      "number of url = 2256\n",
      "number of url = 2321\n",
      "number of url = 2332\n",
      "number of url = 2354\n",
      "number of url = 2459\n",
      "number of url = 2490\n",
      "number of url = 2674\n",
      "number of url = 2701\n",
      "number of url = 2765\n",
      "number of url = 2849\n",
      "number of url = 2919\n",
      "number of url = 2985\n",
      "number of url = 2997\n",
      "number of url = 3017\n",
      "number of url = 3054\n",
      "number of url = 3103\n",
      "number of url = 3163\n",
      "number of url = 3175\n",
      "number of url = 3206\n",
      "number of url = 3249\n",
      "number of url = 3276\n",
      "number of url = 3310\n",
      "number of url = 3322\n",
      "number of url = 3333\n",
      "number of url = 3362\n",
      "number of url = 3396\n",
      "number of url = 3441\n",
      "number of url = 3463\n",
      "number of url = 3481\n",
      "number of url = 3482\n",
      "number of url = 3491\n",
      "number of url = 3491\n",
      "number of url = 3493\n",
      "number of url = 3559\n",
      "number of url = 3591\n",
      "number of url = 3608\n",
      "number of url = 3628\n",
      "number of url = 3703\n",
      "number of url = 3765\n",
      "number of url = 3767\n",
      "number of url = 3905\n",
      "number of url = 4011\n",
      "number of url = 4062\n",
      "number of url = 4108\n",
      "number of url = 4122\n",
      "number of url = 4134\n",
      "number of url = 4151\n",
      "number of url = 4276\n",
      "number of url = 4298\n",
      "number of url = 4313\n",
      "number of url = 4333\n",
      "number of url = 4361\n",
      "number of url = 4372\n",
      "number of url = 4413\n",
      "number of url = 4446\n",
      "number of url = 4455\n",
      "number of url = 4689\n",
      "number of url = 4746\n",
      "number of url = 4792\n",
      "number of url = 4821\n",
      "number of url = 4903\n",
      "number of url = 4962\n",
      "number of url = 4973\n",
      "number of url = 4996\n",
      "number of url = 5038\n",
      "number of url = 5104\n",
      "number of url = 5132\n",
      "number of url = 5155\n",
      "number of url = 5170\n",
      "number of url = 5197\n",
      "number of url = 5227\n",
      "number of url = 5244\n",
      "number of url = 5313\n",
      "number of url = 5348\n",
      "number of url = 5365\n",
      "number of url = 5379\n",
      "number of url = 5467\n",
      "number of url = 5490\n",
      "number of url = 5517\n",
      "number of url = 5559\n",
      "number of url = 5613\n",
      "number of url = 5640\n",
      "number of url = 5650\n",
      "number of url = 5745\n",
      "number of url = 5788\n",
      "number of url = 5961\n",
      "number of url = 6030\n",
      "number of url = 6101\n",
      "number of url = 6135\n",
      "number of url = 6220\n",
      "number of url = 6253\n",
      "number of url = 6275\n",
      "number of url = 6320\n",
      "number of url = 6340\n",
      "number of url = 6368\n",
      "number of url = 6374\n",
      "number of url = 6400\n",
      "number of url = 6414\n",
      "number of url = 6425\n",
      "number of url = 6434\n",
      "number of url = 6469\n",
      "number of url = 6551\n",
      "number of url = 6572\n",
      "number of url = 6603\n",
      "number of url = 6691\n",
      "number of url = 6709\n",
      "number of url = 6782\n",
      "number of url = 6815\n",
      "number of url = 6822\n",
      "number of url = 6863\n",
      "number of url = 6892\n",
      "number of url = 6905\n",
      "number of url = 6947\n",
      "number of url = 6976\n",
      "number of url = 7011\n",
      "number of url = 7020\n",
      "number of url = 7047\n",
      "number of url = 7074\n",
      "number of url = 7083\n",
      "number of url = 7098\n",
      "number of url = 7119\n",
      "number of url = 7156\n",
      "number of url = 7177\n",
      "number of url = 7177\n",
      "number of url = 7177\n",
      "number of url = 7217\n",
      "number of url = 7225\n",
      "number of url = 7260\n",
      "number of url = 7263\n",
      "number of url = 7289\n",
      "number of url = 7289\n",
      "number of url = 7289\n",
      "number of url = 7334\n",
      "number of url = 7341\n",
      "number of url = 7413\n",
      "number of url = 7413\n",
      "number of url = 7475\n",
      "number of url = 7481\n",
      "number of url = 7500\n",
      "number of url = 7510\n",
      "number of url = 7510\n",
      "number of url = 7525\n",
      "number of url = 7538\n",
      "number of url = 7554\n",
      "number of url = 7559\n",
      "number of url = 7628\n",
      "number of url = 7628\n",
      "number of url = 7635\n",
      "number of url = 7635\n",
      "number of url = 7635\n",
      "number of url = 7648\n",
      "number of url = 7648\n",
      "number of url = 7648\n",
      "number of url = 7685\n",
      "number of url = 7690\n",
      "number of url = 7709\n",
      "number of url = 7735\n",
      "number of url = 7819\n",
      "number of url = 7874\n",
      "number of url = 7893\n",
      "number of url = 7907\n",
      "number of url = 7911\n",
      "number of url = 7949\n",
      "number of url = 7984\n",
      "number of url = 8001\n",
      "number of url = 8001\n",
      "number of url = 8005\n",
      "number of url = 8022\n",
      "number of url = 8032\n",
      "number of url = 8039\n",
      "number of url = 8062\n",
      "number of url = 8069\n",
      "number of url = 8075\n",
      "number of url = 8107\n",
      "number of url = 8141\n",
      "finish 200\n",
      "finish 400\n",
      "Error fetching URL https://www.domain.com.au/22-200-wattletree-road-malvern-vic-3144-17185487: 500 Server Error: Internal Server Error for url: https://www.domain.com.au/22-200-wattletree-road-malvern-vic-3144-17185487\n",
      "finish 600\n",
      "finish 800\n",
      "finish 1000\n",
      "finish 1200\n",
      "finish 1400\n",
      "finish 1600\n",
      "finish 1800\n",
      "finish 2000\n",
      "finish 2200\n",
      "finish 2400\n",
      "finish 2600\n",
      "finish 2800\n",
      "finish 3000\n",
      "finish 3200\n",
      "finish 3400\n",
      "finish 3600\n",
      "finish 3800\n",
      "finish 4000\n",
      "finish 4200\n",
      "finish 4400\n",
      "finish 4600\n",
      "finish 4800\n",
      "finish 5000\n",
      "finish 5200\n",
      "finish 5400\n",
      "finish 5600\n",
      "finish 5800\n",
      "finish 6000\n",
      "finish 6200\n",
      "finish 6400\n",
      "finish 6600\n",
      "No JSON-LD data found in https://www.domain.com.au/1301-151-berkeley-street-melbourne-vic-3000-17186246\n",
      "digitalData script not found in https://www.domain.com.au/1301-151-berkeley-street-melbourne-vic-3000-17186246\n",
      "finish 6800\n",
      "finish 7000\n",
      "finish 7200\n",
      "finish 7400\n",
      "finish 7600\n",
      "finish 7800\n",
      "finish 8000\n"
     ]
    }
   ],
   "source": [
    "output_path = \"../data/landing/properties.csv\"\n",
    "#base_url_l = ['https://www.domain.com.au/rent/cranbourne-vic-3977/?ssubs=0&page=']\n",
    "page_l = range(1,31)\n",
    "properties = scrape_properties(base_url_l, page_l, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebf736bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been created at: ../data/landing/properties.csv\n"
     ]
    }
   ],
   "source": [
    "convert_to_csv(properties, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c597da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
