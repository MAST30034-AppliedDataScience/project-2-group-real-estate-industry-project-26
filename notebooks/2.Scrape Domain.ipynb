{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25493ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from time import sleep\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73d0e32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_property_info(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching URL {url}: {e}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "    property_info = {\n",
    "        'price': None,\n",
    "        'area': 'Not available',\n",
    "        'bedrooms': None,\n",
    "        'bathrooms': None,\n",
    "        'parking': None,\n",
    "        'agency': {'id': None, 'name': None},\n",
    "        'nbn_type': None,\n",
    "        'property_type': None,\n",
    "        'location': None,\n",
    "        'nearby_schools': [],\n",
    "        'age_distribution': {},\n",
    "        'geo': None \n",
    "    }\n",
    "\n",
    "    json_ld = soup.find('script', type='application/ld+json')\n",
    "    if json_ld:\n",
    "        try:\n",
    "            json_data = json.loads(json_ld.string)\n",
    "            if not isinstance(json_data, list):\n",
    "                json_data = [json_data]\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON-LD data from {url}: {e}\")\n",
    "            print(\"JSON-LD content:\", json_ld.string[:500])\n",
    "            json_data = []\n",
    "    else:\n",
    "        print(f\"No JSON-LD data found in {url}\")\n",
    "        json_data = []\n",
    "\n",
    "    digital_data_script = soup.find('script', string=re.compile('var digitalData'))\n",
    "    if digital_data_script:\n",
    "        digital_data_match = re.search(r'var digitalData = (.+?);', digital_data_script.string, re.DOTALL)\n",
    "        if digital_data_match:\n",
    "            try:\n",
    "                digital_data = json.loads(digital_data_match.group(1))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding digitalData from {url}: {e}\")\n",
    "                print(\"digitalData content:\", digital_data_match.group(1)[:500])\n",
    "                digital_data = {}\n",
    "        else:\n",
    "            print(f\"digitalData pattern not found in {url}\")\n",
    "            digital_data = {}\n",
    "    else:\n",
    "        print(f\"digitalData script not found in {url}\")\n",
    "        digital_data = {}\n",
    "\n",
    "    page_info = digital_data.get('page', {})\n",
    "    if not isinstance(page_info, dict):\n",
    "        print(f\"Unexpected 'page' structure in digital_data: {type(page_info)}\")\n",
    "        page_info = {}\n",
    "\n",
    "    property_data = page_info.get('pageInfo', {}).get('property', {})\n",
    "    if not isinstance(property_data, dict):\n",
    "        print(f\"Unexpected 'property' structure in digital_data: {type(property_data)}\")\n",
    "        property_data = {}\n",
    "\n",
    "    property_info['price'] = property_data.get('price')\n",
    "    property_info['area'] = 'Not available'\n",
    "    property_info['bedrooms'] = property_data.get('bedrooms')\n",
    "    property_info['bathrooms'] = property_data.get('bathrooms')\n",
    "    property_info['parking'] = property_data.get('parking')\n",
    "    property_info['agency'] = {\n",
    "        'id': property_data.get('agencyId'),\n",
    "        'name': property_data.get('agency')\n",
    "    }\n",
    "    property_info['nbn_type'] = page_info.get('pageInfo', {}).get('nbnDetails')\n",
    "    property_info['property_type'] = property_data.get('primaryPropertyType')\n",
    "    \n",
    "    if not property_info['price']:\n",
    "        price_element = soup.select_one('.css-twgrok[data-testid=\"listing-details__summary-title\"] span')\n",
    "        if price_element:\n",
    "            price_text = price_element.text.strip()\n",
    "            property_info['price'] = price_text.split()[0] if price_text else None\n",
    "\n",
    "    for item in json_data:\n",
    "        if isinstance(item, dict) and item.get('@type') == 'Event' and 'location' in item:\n",
    "            geo = item['location'].get('geo', {})\n",
    "            property_info['geo'] = {\n",
    "                'latitude': geo.get('latitude'),\n",
    "                'longitude': geo.get('longitude')\n",
    "            }\n",
    "            break\n",
    "    \n",
    "    for item in json_data:\n",
    "        if item.get('@type') == 'Residence':\n",
    "            address = item.get('address', {})\n",
    "            property_info['location'] = {\n",
    "                'streetAddress': address.get('streetAddress'),\n",
    "                'addressLocality': address.get('addressLocality'),\n",
    "                'addressRegion': address.get('addressRegion'),\n",
    "                'postalCode': address.get('postalCode')\n",
    "            }\n",
    "            break        \n",
    "            \n",
    "    if not property_info['geo']:\n",
    "        directions_link = soup.find('a', attrs={'aria-label': lambda x: x and 'Directions' in x})\n",
    "        if directions_link and 'href' in directions_link.attrs:\n",
    "            href = directions_link['href']\n",
    "            coordinates = href.split('destination=')[-1].split('&')[0]\n",
    "            if ',' in coordinates:\n",
    "                lat, lng = coordinates.split(',')\n",
    "                property_info['geo'] = {\n",
    "                    'latitude': float(lat),\n",
    "                    'longitude': float(lng)\n",
    "                }\n",
    "    \n",
    "    schools = []\n",
    "    school_elements = soup.find_all(['div', 'label'], class_=['css-1eyghyo', 'domain-checkbox'])\n",
    "    for school in school_elements:\n",
    "        school_name = school.find(['h4', 'div'], class_=['css-5w5cop', 'domain-checkbox__label'])\n",
    "        if school_name:\n",
    "            schools.append(school_name.text.strip())\n",
    "    property_info['nearby_schools'] = list(set(schools))\n",
    "\n",
    "    age_distribution = {}\n",
    "    age_rows = soup.find_all('tr', class_='css-1a43shy')\n",
    "    for row in age_rows:\n",
    "        age_range = row.find('td', class_='css-1srjr3j')\n",
    "        percentage = row.find('div', class_='css-199ul8s')\n",
    "        if age_range and percentage:\n",
    "            age_distribution[age_range.text.strip()] = percentage.text.strip()\n",
    "    property_info['age_distribution'] = age_distribution\n",
    "\n",
    "    return property_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3df4a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_property_urls(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    property_urls = []\n",
    "\n",
    "    json_ld_scripts = soup.find_all('script', type='application/ld+json')\n",
    "    for script in json_ld_scripts:\n",
    "        try:\n",
    "            data = json.loads(script.string)\n",
    "            if isinstance(data, list):\n",
    "                for item in data:\n",
    "                    if item.get('@type') == 'Event' and 'url' in item:\n",
    "                        property_urls.append(item['url'])\n",
    "            elif isinstance(data, dict) and data.get('@type') == 'Event' and 'url' in data:\n",
    "                property_urls.append(data['url'])\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "    all_links = soup.find_all('a', href=True)\n",
    "    domain_pattern = re.compile(r'https?://www\\.domain\\.com\\.au/.*-\\d+$')\n",
    "\n",
    "    for link in all_links:\n",
    "        href = link['href']\n",
    "        if domain_pattern.match(href):\n",
    "            property_urls.append(href)\n",
    "        elif href.startswith('/') and '-' in href and href.split('-')[-1].isdigit():\n",
    "            full_url = f\"https://www.domain.com.au{href}\"\n",
    "            if full_url.endswith('/'):\n",
    "                property_urls.append(full_url)\n",
    "            else:\n",
    "                property_urls.append(full_url + '/')\n",
    "\n",
    "    \n",
    "\n",
    "    property_urls = list(set(property_urls))\n",
    "    \n",
    "    filtered_urls = [\n",
    "        url for url in property_urls \n",
    "        if re.search(r'/[\\w-]+-\\d+$', url) and 'suburb-profile' not in url\n",
    "    ]\n",
    "    \n",
    "    return filtered_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58b5e67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_csv(property_l, output_path):\n",
    "    if isinstance(property_l, str):\n",
    "        property_l = ast.literal_eval(property_l)\n",
    "        \n",
    "    property_l = [p for p in property_l if p is not None]\n",
    "\n",
    "    if not property_l:\n",
    "        print(\"No valid properties to write to CSV.\")\n",
    "        return\n",
    "    \n",
    "    fieldnames = set()\n",
    "    for property_info in property_l:\n",
    "        if isinstance(property_info, dict):\n",
    "            fieldnames.update(property_info.keys())\n",
    "        else:\n",
    "            print(f\"Skipping non-dict item: {property_info}\")\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        \n",
    "        for property_info in property_l:\n",
    "            writer.writerow(property_info)\n",
    "\n",
    "    print(f\"CSV file has been created at: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0cc039",
   "metadata": {},
   "source": [
    "\n",
    "output_path = \"../data/landing/properties.csv\"\n",
    "\n",
    "#### 调用函数\n",
    "convert_property_list_to_csv(a, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a70f20de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_properties(base_url, page_l, output_path):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "    url_l = []\n",
    "    property_l = []\n",
    "    \n",
    "    for base_url in base_url_l:\n",
    "        response = requests.get(base_url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            for page in page_l:\n",
    "                url = base_url + str(page)\n",
    "                try:\n",
    "                    response = requests.get(url, headers=headers, timeout=10)\n",
    "                    response.raise_for_status()\n",
    "                    new_urls = extract_property_urls(url)\n",
    "                    if not new_urls:\n",
    "                        break\n",
    "                    url_l.extend(new_urls)\n",
    "                except RequestException as e:\n",
    "                    break\n",
    "          \n",
    "           \n",
    "        print(f\"number of url = {len(url_l)}\")\n",
    "    url_l = list(set(url_l))\n",
    "    \n",
    "    cont = 0\n",
    "    for a_url in url_l:\n",
    "        cont += 1\n",
    "        property_l.append(extract_property_info(a_url))\n",
    "        if cont % 200 == 0:\n",
    "            print(f\"finish {cont}\")\n",
    "    \n",
    "    return property_l    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6726ecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url_l = []\n",
    "with open('../data/landing/victoria_suburbs_postcodes.csv', 'r', newline='', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  \n",
    "    for row in reader:\n",
    "        if len(row) >= 2:\n",
    "            suburb, postcode = row[0], row[1]\n",
    "            suburb_url = suburb.lower().replace(' ', '-')\n",
    "            url = f\"https://www.domain.com.au/rent/{suburb_url}-vic-{postcode}/?ssubs=0&page=\"\n",
    "            base_url_l.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb71af8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of url = 893\n",
      "number of url = 893\n",
      "number of url = 932\n",
      "number of url = 1032\n",
      "number of url = 1136\n",
      "number of url = 1523\n",
      "number of url = 1720\n",
      "number of url = 1866\n",
      "number of url = 1881\n",
      "number of url = 1942\n",
      "number of url = 1968\n",
      "number of url = 1982\n",
      "number of url = 2023\n",
      "number of url = 2052\n",
      "number of url = 2072\n",
      "number of url = 2111\n",
      "number of url = 2126\n",
      "number of url = 2245\n",
      "number of url = 2276\n",
      "number of url = 2327\n",
      "number of url = 2365\n",
      "number of url = 2392\n",
      "number of url = 2422\n",
      "number of url = 2435\n",
      "number of url = 2500\n",
      "number of url = 2525\n",
      "number of url = 2562\n",
      "number of url = 2625\n",
      "number of url = 2635\n",
      "number of url = 2658\n",
      "number of url = 2763\n",
      "number of url = 2795\n",
      "number of url = 2984\n",
      "number of url = 3003\n",
      "number of url = 3072\n",
      "number of url = 3151\n",
      "number of url = 3217\n",
      "number of url = 3281\n",
      "number of url = 3289\n",
      "number of url = 3309\n",
      "number of url = 3343\n",
      "number of url = 3397\n",
      "number of url = 3454\n",
      "number of url = 3468\n",
      "number of url = 3500\n",
      "number of url = 3547\n",
      "number of url = 3576\n",
      "number of url = 3609\n",
      "number of url = 3630\n",
      "number of url = 3643\n",
      "number of url = 3680\n",
      "number of url = 3715\n",
      "number of url = 3759\n",
      "number of url = 3778\n",
      "number of url = 3794\n",
      "number of url = 3796\n",
      "number of url = 3809\n",
      "number of url = 3809\n",
      "number of url = 3811\n",
      "number of url = 3882\n",
      "number of url = 3914\n",
      "number of url = 3930\n",
      "number of url = 3948\n",
      "number of url = 4028\n",
      "number of url = 4089\n",
      "number of url = 4091\n",
      "number of url = 4241\n",
      "number of url = 4324\n",
      "number of url = 4373\n",
      "number of url = 4414\n",
      "number of url = 4430\n",
      "number of url = 4445\n",
      "number of url = 4467\n",
      "number of url = 4597\n",
      "number of url = 4625\n",
      "number of url = 4644\n",
      "number of url = 4668\n",
      "number of url = 4697\n",
      "number of url = 4706\n",
      "number of url = 4745\n",
      "number of url = 4777\n",
      "number of url = 4788\n",
      "number of url = 5023\n",
      "number of url = 5082\n",
      "number of url = 5126\n",
      "number of url = 5153\n",
      "number of url = 5240\n",
      "number of url = 5291\n",
      "number of url = 5304\n",
      "number of url = 5329\n",
      "number of url = 5369\n",
      "number of url = 5443\n",
      "number of url = 5471\n",
      "number of url = 5487\n",
      "number of url = 5500\n",
      "number of url = 5531\n",
      "number of url = 5566\n",
      "number of url = 5578\n",
      "number of url = 5643\n",
      "number of url = 5677\n",
      "number of url = 5693\n",
      "number of url = 5707\n",
      "number of url = 5801\n",
      "number of url = 5824\n",
      "number of url = 5856\n",
      "number of url = 5906\n",
      "number of url = 5967\n",
      "number of url = 5997\n",
      "number of url = 6014\n",
      "number of url = 6112\n",
      "number of url = 6155\n",
      "number of url = 6328\n",
      "number of url = 6398\n",
      "number of url = 6470\n",
      "number of url = 6503\n",
      "number of url = 6584\n",
      "number of url = 6617\n",
      "number of url = 6641\n",
      "number of url = 6688\n",
      "number of url = 6705\n",
      "number of url = 6731\n",
      "number of url = 6738\n",
      "number of url = 6764\n",
      "number of url = 6775\n",
      "number of url = 6786\n",
      "number of url = 6792\n",
      "number of url = 6827\n",
      "number of url = 6916\n",
      "number of url = 6940\n",
      "number of url = 6970\n",
      "number of url = 7060\n",
      "number of url = 7082\n",
      "number of url = 7156\n",
      "number of url = 7189\n",
      "number of url = 7196\n",
      "number of url = 7233\n",
      "number of url = 7267\n",
      "number of url = 7279\n",
      "number of url = 7322\n",
      "number of url = 7350\n",
      "number of url = 7383\n",
      "number of url = 7391\n",
      "number of url = 7417\n",
      "number of url = 7443\n",
      "number of url = 7453\n",
      "number of url = 7468\n",
      "number of url = 7489\n",
      "number of url = 7527\n",
      "number of url = 7550\n",
      "number of url = 7550\n",
      "number of url = 7550\n",
      "number of url = 7591\n",
      "number of url = 7599\n",
      "number of url = 7632\n",
      "number of url = 7636\n",
      "number of url = 7662\n",
      "number of url = 7662\n",
      "number of url = 7662\n",
      "number of url = 7704\n",
      "number of url = 7713\n",
      "number of url = 7773\n",
      "number of url = 7773\n",
      "number of url = 7833\n",
      "number of url = 7839\n",
      "number of url = 7853\n",
      "number of url = 7862\n",
      "number of url = 7862\n",
      "number of url = 7878\n",
      "number of url = 7893\n",
      "number of url = 7911\n",
      "number of url = 7922\n",
      "number of url = 7995\n",
      "number of url = 7995\n",
      "number of url = 8001\n",
      "number of url = 8001\n",
      "number of url = 8001\n",
      "number of url = 8015\n",
      "number of url = 8015\n",
      "number of url = 8015\n",
      "number of url = 8053\n",
      "number of url = 8058\n",
      "number of url = 8076\n",
      "number of url = 8102\n",
      "number of url = 8181\n",
      "number of url = 8235\n",
      "number of url = 8254\n",
      "number of url = 8267\n",
      "number of url = 8273\n",
      "number of url = 8306\n",
      "number of url = 8339\n",
      "number of url = 8351\n",
      "number of url = 8351\n",
      "number of url = 8355\n",
      "number of url = 8373\n",
      "number of url = 8385\n",
      "number of url = 8391\n",
      "number of url = 8411\n",
      "number of url = 8418\n",
      "number of url = 8425\n",
      "number of url = 8457\n",
      "number of url = 8491\n",
      "finish 200\n",
      "finish 400\n",
      "finish 600\n",
      "finish 800\n",
      "finish 1000\n",
      "finish 1200\n",
      "finish 1400\n",
      "finish 1600\n",
      "finish 1800\n",
      "finish 2000\n",
      "finish 2200\n",
      "finish 2400\n",
      "finish 2600\n",
      "finish 2800\n",
      "finish 3000\n",
      "finish 3200\n",
      "finish 3400\n",
      "finish 3600\n",
      "finish 3800\n",
      "finish 4000\n",
      "No JSON-LD data found in https://www.domain.com.au/306-330-lygon-street-brunswick-east-vic-3057-17183032\n",
      "digitalData script not found in https://www.domain.com.au/306-330-lygon-street-brunswick-east-vic-3057-17183032\n",
      "finish 4200\n",
      "finish 4400\n",
      "Error fetching URL https://www.domain.com.au/22-200-wattletree-road-malvern-vic-3144-17185487: 500 Server Error: Internal Server Error for url: https://www.domain.com.au/22-200-wattletree-road-malvern-vic-3144-17185487\n",
      "finish 4600\n",
      "finish 4800\n",
      "finish 5000\n",
      "finish 5200\n",
      "finish 5400\n",
      "finish 5600\n",
      "finish 5800\n",
      "finish 6000\n",
      "finish 6200\n",
      "finish 6400\n",
      "finish 6600\n",
      "finish 6800\n",
      "finish 7000\n",
      "finish 7200\n",
      "finish 7400\n",
      "finish 7600\n",
      "finish 7800\n",
      "finish 8000\n",
      "finish 8200\n",
      "finish 8400\n"
     ]
    }
   ],
   "source": [
    "output_path = \"../data/landing/properties.csv\"\n",
    "page_l = range(1,51)\n",
    "properties = scrape_properties(base_url_l, page_l, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebf736bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been created at: ../data/landing/properties.csv\n"
     ]
    }
   ],
   "source": [
    "convert_to_csv(properties, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c597da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
